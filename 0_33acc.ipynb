{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katie291100/learn-nlp/blob/main/0_33acc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7q4XP7hKrzK",
        "outputId": "962328cb-b7ed-4510-e64e-b15896b53d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHrCiGBD8Nub"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDb3YbZCpxKP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR4-4_5WGK4L",
        "outputId": "8995571b-3377-4327-e4af-04e536f1e3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['talendesb.csv', 'springxd.csv', 'clover.csv', 'aptanastudio.csv', 'datamanagement.csv', 'mulestudio.csv', 'moodle.csv', 'jirasoftware.csv', 'talenddataquality.csv', 'bamboo.csv', 'duracloud.csv', 'mesos.csv', 'usergrid.csv', 'titanium.csv', 'mule.csv', 'appceleratorstudio.csv']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "folder_path = 'drive/MyDrive/storypoint/IEEE TSE2018/dataset'\n",
        "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
        "print(csv_files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vymlv83w85q3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll_BWjFc8ZlK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "gpu_avail = torch.cuda.is_available()\n",
        "print(f\"Is the GPU available? {gpu_avail}\")\n",
        "device=torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYh7Fx4-J1VW"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AutoTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as nnf\n",
        "\n",
        "# Set up parameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 5\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "learning_rate = 3e-5\n",
        "device = 'cuda'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            x = self.fc1(output.pooler_output)\n",
        "            x=self.dropout(x)\n",
        "            x = nnf.relu(self.fc2(x))\n",
        "\n",
        "            x=self.dropout(x)\n",
        "            logits = nnf.relu(self.fc3(x))\n",
        "            return logits\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOMDOVFQQC7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLBrVS3RwonL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRKOLAUIJXK6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataframes.append(df)\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I54IlsglwZ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXwe_GoieyCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df"
      ],
      "metadata": {
        "id": "l5mK0K7wFJaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C-I3K0qLOrR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "combined_df = combined_df.query(\"description.notna()\").get([\"storypoint\", \"description\", \"title\"])\n",
        "df1 = combined_df.query(\"storypoint in [1]\")[:2000]\n",
        "df2 = combined_df.query(\"storypoint in [2]\")[:2000]\n",
        "df3 = combined_df.query(\"storypoint in [3]\")[:2000]\n",
        "df4 = combined_df.query(\"storypoint in [5]\")[:2000]\n",
        "df5 = combined_df.query(\"storypoint in [8]\")[:2000]\n",
        "\n",
        "combined_df = df1.append(df2).append(df3).append(df4).append(df5)\n",
        "combined_df['description'] = combined_df['title'] + ' ' + combined_df['description']\n",
        "combined_df = combined_df.drop(columns=['title'])\n",
        "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# combined_df[\"storypoint\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df"
      ],
      "metadata": {
        "id": "pnGbxXosK3rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "QS5la09YbWTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiC3kNIS4GDX"
      },
      "outputs": [],
      "source": [
        "combined_df[\"storypoint\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipMJ8E1Gw_Ki"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "          super().__init__()\n",
        "          self.texts = texts\n",
        "          self.labels = np.array(list(map(lambda x: [1,2,3,5,8].index(x), list(labels))))\n",
        "          self.tokenizer = tokenizer\n",
        "          self.max_length = max_length\n",
        "  def __len__(self):\n",
        "      return len(self.texts)\n",
        "  def __getitem__(self, idx):\n",
        "      text = self.texts[idx]\n",
        "      label = torch.tensor(self.labels[idx]).to(device)\n",
        "      encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "      encoding = encoding.to(\"cuda\")\n",
        "      return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': label}\n",
        "\n",
        "train, testds = train_test_split(combined_df, test_size=0.2)\n",
        "print(len(train['storypoint'].values))\n",
        "print(len(testds['description'].values))\n",
        "\n",
        "train_dataset = TextClassificationDataset(train['description'].values, train['storypoint'].values, tokenizer, 256)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWtwFASReGFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoXH_9Zq8eBn"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(train['storypoint'].values)\n",
        "print(np.array(list(map(lambda x: [1,2,3,5,8].index(x), list(train['storypoint'].values)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL_YSOztA2xj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "train_dataset = TextClassificationDataset(train['description'].values, train['storypoint'].values, tokenizer, 256)\n",
        "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "test_dataset = TextClassificationDataset(testds['description'].values, testds['storypoint'].values, tokenizer, 256)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "def test():\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actual_labels = []\n",
        "  with torch.no_grad():\n",
        "      for i, batch in enumerate(test_dataloader):\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['label'].to(device)\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "          _, preds = torch.max(outputs, dim=1)\n",
        "          predictions.extend(preds.cpu().tolist())\n",
        "          actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "  print(classification_report(actual_labels, predictions))\n",
        "\n",
        "# loss = 0\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, batch in enumerate(data_loader):\n",
        "      optimizer.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "      loss.backward()\n",
        "      epoch_loss += loss.item()\n",
        "      optimizer.step()\n",
        "  print(\"loss: \",epoch_loss/len(data_loader))\n",
        "  test()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actual_labels = []\n",
        "  with torch.no_grad():\n",
        "      for i, batch in enumerate(test_dataloader):\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['label'].to(device)\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "          preds = torch.argmax(outputs, dim=1)\n",
        "          predictions.extend(preds.cpu().tolist())\n",
        "          actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "\n",
        "  print(classification_report(actual_labels, predictions))\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "BWBBtYpR7E3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9unqkTX10-7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3e-4"
      ],
      "metadata": {
        "id": "Hn3NKA2v0wSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhxSZzPx0hop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iFCpPNv0S_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taAohIXR4Gfx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hmAgMbwGknS"
      },
      "outputs": [],
      "source": [
        "report = classification_report(actual_labels, predictions)\n",
        "print(report)\n",
        "print(true_labels)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpYhkfTU0SfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10rrkeG0vCboc7EU2p5EbaAKo704R20OS",
      "authorship_tag": "ABX9TyMot7dIbS6qsUmPVL5OrZt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}